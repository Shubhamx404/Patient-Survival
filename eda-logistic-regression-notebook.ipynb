{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2972359,"sourceType":"datasetVersion","datasetId":1789262}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook presents the development of a classification model aimed at predicting in-hospital mortality for admitted patients. It is structured into five sections:\n\n1. Data Loading\n2. Exploratory Data Analysis & Data Cleaning\n3. Data Pre-Processing\n4. Logistic Regression Model\n5. Conclusion","metadata":{}},{"cell_type":"code","source":"# import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n\nimport warnings as w\nw.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ## 1. Data Loading","metadata":{}},{"cell_type":"code","source":"# Function for Importing and analyzing raw data\ndef load_data(data_path, dataset_name):\n    \n    pd.set_option('display.max_rows', None)\n    pd.set_option('display.max_columns', None) \n    print(\"\\n\"+\"*\"*100+\"\\n\")\n    print(f\"DATASET NAME: {dataset_name}\\n\")\n    raw_data = pd.read_csv(data_path + dataset_name + '.csv')\n    display(raw_data.head())\n    print(\"\\n\"+\"-\"*100+\"\\n\")\n    print(f\"SHAPE: {raw_data.shape}\")\n    print(\"\\n\"+\"-\"*100+\"\\n\")\n    print(f\"{raw_data.info()}\")\n    print(\"\\n\"+\"-\"*100+\"\\n\")\n    print(\"Duplicate rows in Data:\")\n    display(raw_data[raw_data.duplicated()])\n    print(\"\\n\"+\"-\"*100+\"\\n\")\n    print(\"Data Summary:\\n\")\n    display(raw_data.describe(include = 'all'))\n    print(\"\\n\"+\"-\"*100+\"\\n\")\n    return raw_data\n\n\nraw_data = load_data('../input/patient/', 'dataset')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Conclusions from above data information:\n\n* Above dataset has 91713 data examples with 85 variables, which include the dependent variable.\n* There are three types of columns in data: Float, Integer and Object.\n* No duplicate rows are present in the dataset.\n* The column named 'Unnamed: 83' contains solely null values, suggesting it should be dropped from the analysis.\n* 'encounter_id', 'patient_id' and 'hospital_id' are unlikely to be relevant to patient survival and should be removed before further analysis.","metadata":{}},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis & Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Drop columns 'encounter_id' ,'patient_id' ,'hospital_id' and 'Unnamed: 83'\nraw_data.drop(['encounter_id' ,'patient_id' ,'hospital_id','Unnamed: 83'],axis =1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Listing Numerical and Categorical variables separately for exploratory data analysis\nnumerical_col = []\ncategorical_col = []\nfor col in raw_data.columns:\n    if (raw_data[col].dtype==int)or (raw_data[col].dtype==float):\n        numerical_col.append(col)\n    elif (raw_data[col].dtype==object):\n        categorical_col.append(col)\nprint (f'Numerical columns:\\n\\n {numerical_col}')\nprint(\"\\n\"+\"-\"*150+\"\\n\")\nprint (f'Categorical columns:\\n\\n {categorical_col}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plot Numerical columns\nfor col in numerical_col:\n    plt.figure(figsize=(8,4))\n    sns.histplot(raw_data[col])\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plot Categorical columns\nfor col in categorical_col:\n    plt.figure(figsize=(6,4))\n    count_plot = sns.countplot(x=raw_data[col])\n    #rotate x-axis labels\n    count_plot.set_xticklabels(count_plot.get_xticklabels(), rotation=45)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset features normal distributions for major variables and no observed gender bias. However, bias is expected in other categorical variables, likely due to their relevance to critical patient characteristics.","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Pre-processing","metadata":{}},{"cell_type":"code","source":"#Check null data percentage\nraw_data.isnull().sum()*100/len(raw_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset exhibits missing values in several variables, each suggesting potential errors in data collection. To preserve valuable information, imputation rather than discarding data is recommended. Here's a breakdown of how to handle missing values for each variable:\n\n**Age:** Age variable exhibits 4.61% missing values, suggesting that the absence of data is likely due to error. Therefore, it is advisable to impute these missing values rather than discarding potentially valuable data. Since age distribution is left-skewed, replacing missing values with the median age is recommended.\n\n**BMI:** BMI variable displays a 3.73% incidence of missing values, indicating error similar to age variable. Hence, it is crucial to employ an appropriate technique to address these gaps in the data. Examination of the distribution of the BMI variable reveals a right-skewed pattern. Therefore, due to the skewed nature of the data, it is advisable to substitute the missing values with the median BMI value.\n\n**Height:** Height variable has 1.45% missing values, likely due to oversight. As it approximates a normal distribution, filling missing values with the mean is recommended.\n\n**Weight:** Weight variable has 2.96% missing values. Considering its right-skewed distribution, it's advisable to replace missing values with the median.\n\nFor the remaining variables with missing values, such as medical test readings, imputation techniques may not be suitable due to variations among patients. It's recommended to drop these missing data points to maintain the integrity of the analysis.","metadata":{}},{"cell_type":"code","source":"#Imputing missing values 'height' column\nimputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nraw_data['height'] = imputer_mean.fit_transform(raw_data[['height']])\n\n#Imputing missing values for 'age', 'bmi', and 'weight' columns\nimputer_median = SimpleImputer(missing_values=np.nan, strategy='median')\nraw_data['age'] = imputer_median.fit_transform(raw_data[['age']])\nraw_data['bmi'] = imputer_median.fit_transform(raw_data[['bmi']])\nraw_data['weight'] = imputer_median.fit_transform(raw_data[['weight']])\n\n# Removing all the null values for other remaining variables\nraw_data.dropna(inplace = True)\n\n#creating a copy of this cleaned data\ndata = raw_data\ndata.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert categorical variables to dummy variables\ndata = pd.get_dummies(data, columns=categorical_col, drop_first=True, dtype=int)\n\n#Print updated data\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining independent variables and dependent variable\ny = data['hospital_death'].values\nx = data.drop(['hospital_death'], axis=1).values\nprint(data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform train test split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Scaling\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Logistic Regression Model","metadata":{}},{"cell_type":"markdown","source":"Logistic regression is a fundamental yet powerful tool in the realm of classification. It's widely used because of its simplicity, interpretability, and effectiveness, especially for binary classification problems. Let us dive into implementing a basic logistic regression model.","metadata":{}},{"cell_type":"code","source":"# Training Logistic Regression Model using x_train\nclassifier = LogisticRegression(random_state = 42)\nclassifier.fit(x_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predicting y value for x_test data using above model\ny_pred = classifier.predict(x_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dataframe creation for actual and predicted values of dependent variable\npredicted_data = { 'hospital_death': y_test,'predicted_hospital_death':y_pred}\npredicted_data = pd.DataFrame(predicted_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Making confusion matrix \nconfusion_matrix_logistic_reg = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(4,3))\nsns.heatmap(confusion_matrix_logistic_reg, annot=True, fmt='d', cmap='Blues')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nprint('\\nClassification Report:\\n')\nprint(classification_report(y_test, y_pred))\nprint('\\nConfusion matrix:\\n')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model's accuracy score reveals its ability to accurately predict 93% of the dependent variable. However, its precision of 0.68 indicates that out of 440 predicted hospital deaths, only 299 were correct. Similarly, the model's recall, at 0.28, highlights its inefficiency, correctly predicting only 299 out of 1066 actual hospital deaths. This disparity between high accuracy and low recall stems from imbalanced data. Adjusting the threshold value for probability can be an effective solution in such cases, potentially enhancing the balance between precision and recall. Since the current model performs well on the majority class but struggles with the minority class, lowering the threshold could make it more sensitive to predicting hospital deaths, potentially improving recall at the expense of precision. However, finding the optimal threshold involves considering trade-offs, as overly lowering it may increase false positives, further reducing precision. Experimentation with various threshold values and evaluation of their impact on precision, recall, and other metrics like F1 score are essential to determining the most suitable balance for the specific problem at hand.  ","metadata":{}},{"cell_type":"code","source":"#Testing different threshold probabilities to calculate F-score\n\ndef label(pos_probs, threshold):\n    return (pos_probs >= threshold).astype('int')\n\n#predict probabities of x_test\ny_pred_prob = classifier.predict_proba(x_test)\n# keep probabilities for the positive outcome only\nprobs = y_pred_prob[:, 1]\n# define thresholds\nthresholds = np.arange(0, 1, 0.01)\n# evaluate each threshold\nscores = [f1_score(y_test, label(probs, t)) for t in thresholds]\n# get best threshold\nix = np.argmax(scores)\nprint(f'Threshold={thresholds[ix]:.3f}, F-Score= {scores[ix]:.5f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With the above results, it is clear that with threshold value at 0.21 logistic model gives highest F-score. Accuracy, precision and recall is calculated below for this model.","metadata":{}},{"cell_type":"code","source":"# Predicting y with threshold value 0.21\ny_pred_new = label(probs, 0.21)\n\n# Making confusion matrix\nconfusion_matrix_logistic_reg_new = confusion_matrix(y_test, y_pred_new)\nfig, ax = plt.subplots(figsize=(4,3))\nsns.heatmap(confusion_matrix_logistic_reg_new, annot=True, fmt='d', cmap='Blues')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nprint('\\nClassification Report:\\n')\nprint(classification_report(y_test, y_pred_new))\nprint('\\nConfusion matrix:\\n')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Conclusion","metadata":{}},{"cell_type":"markdown","source":"In conclusion, this notebook presents a comprehensive approach to developing a classification model for predicting in-hospital mortality among admitted patients. Through thorough data loading, exploratory data analysis, and data preprocessing steps, including handling missing values and dropping irrelevant columns, a logistic regression model was implemented. The model achieved an accuracy score of 93%, but exhibited issues of low precision and recall due to imbalanced data. Adjusting the threshold value for probability improved the model's performance, resulting in the highest F-score at a threshold of 0.21.","metadata":{}}]}